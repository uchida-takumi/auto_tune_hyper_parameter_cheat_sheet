{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# このドキュメントの目的\n",
    "\n",
    "学習パラメータの自動的な探索方法について整理することです。\n",
    "\n",
    "調整するパラメータは機械学習やディープラーニングのものを対象としています。\n",
    "\n",
    "構成としては、自動パラメータの調整ライブラリ　hyperopt　の使用法を中心にしてまとめています。\n",
    "\n",
    "\n",
    "# 参考にした文献\n",
    "\n",
    "https://qiita.com/kenchin110100/items/ac3edb480d789481f134\n",
    "\n",
    "https://qiita.com/nazoking@github/items/f67f92dc60001a43b7dc\n",
    "\n",
    "\n",
    "# hyperopt とは？\n",
    "\n",
    "Distributed Asynchronous Hyperparameter Optimization in Python \n",
    "\n",
    "2019/3/30 時点で　970commits, 43contributors, 3200Star, 587Fork のライブラリ\n",
    "\n",
    "[github] https://github.com/hyperopt/hyperopt\n",
    "\n",
    "[doccumente] http://hyperopt.github.io/hyperopt/\n",
    "\n",
    "[公式wiki] https://github.com/hyperopt/hyperopt/wiki/FMin\n",
    "\n",
    "\n",
    "# インストール\n",
    "\n",
    "> pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 469.86it/s, best loss: 0.003039930524044733]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': -0.05513556496531738}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最も単純なhyperoptの使い方\n",
    "\n",
    "def objective(x):\n",
    "    return x ** 2\n",
    "\n",
    "from hyperopt import fmin, tpe, hp\n",
    "# fmin なので関数を最小化する引数を求める。\n",
    "# fn=最小化する関数\n",
    "# space=関数で定義した変数と、それの探索範囲\n",
    "# algo=探索アルゴリズム\n",
    "# max_evals=探索の繰り返し回数\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=hp.uniform('x', -10, 10),\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100\n",
    "    )\n",
    "\n",
    "best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 473.79it/s, best loss: 0.6661991494458527]\n",
      "'=== best ==='\n",
      "{'x': -0.8162102360579979}\n",
      "''\n",
      "'=== trials.trials ==='\n",
      "[{'book_time': datetime.datetime(2019, 4, 6, 1, 38, 55, 770000),\n",
      "  'exp_key': None,\n",
      "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
      "           'idxs': {'x': [0]},\n",
      "           'tid': 0,\n",
      "           'vals': {'x': [-0.8162102360579979]},\n",
      "           'workdir': None},\n",
      "  'owner': None,\n",
      "  'refresh_time': datetime.datetime(2019, 4, 6, 1, 38, 55, 770000),\n",
      "  'result': {'eval_time': 1554514735.770375,\n",
      "             'loss': 0.6661991494458527,\n",
      "             'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
      "             'status': 'ok'},\n",
      "  'spec': None,\n",
      "  'state': 2,\n",
      "  'tid': 0,\n",
      "  'version': 0},\n",
      " {'book_time': datetime.datetime(2019, 4, 6, 1, 38, 55, 771000),\n",
      "  'exp_key': None,\n",
      "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
      "           'idxs': {'x': [1]},\n",
      "           'tid': 1,\n",
      "           'vals': {'x': [1.9971252494478442]},\n",
      "           'workdir': None},\n",
      "  'owner': None,\n",
      "  'refresh_time': datetime.datetime(2019, 4, 6, 1, 38, 55, 772000),\n",
      "  'result': {'eval_time': 1554514735.77195,\n",
      "             'loss': 3.988509261982114,\n",
      "             'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
      "             'status': 'ok'},\n",
      "  'spec': None,\n",
      "  'state': 2,\n",
      "  'tid': 1,\n",
      "  'version': 0},\n",
      " {'book_time': datetime.datetime(2019, 4, 6, 1, 38, 55, 773000),\n",
      "  'exp_key': None,\n",
      "  'misc': {'cmd': ('domain_attachment', 'FMinIter_Domain'),\n",
      "           'idxs': {'x': [2]},\n",
      "           'tid': 2,\n",
      "           'vals': {'x': [-1.8093289601803857]},\n",
      "           'workdir': None},\n",
      "  'owner': None,\n",
      "  'refresh_time': datetime.datetime(2019, 4, 6, 1, 38, 55, 773000),\n",
      "  'result': {'eval_time': 1554514735.7733529,\n",
      "             'loss': 3.2736712861474357,\n",
      "             'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
      "             'status': 'ok'},\n",
      "  'spec': None,\n",
      "  'state': 2,\n",
      "  'tid': 2,\n",
      "  'version': 0}]\n",
      "''\n",
      "'=== trials.losses() ==='\n",
      "[0.6661991494458527, 3.988509261982114, 3.2736712861474357]\n",
      "''\n",
      "'=== trials.statuses() ==='\n",
      "['ok', 'ok', 'ok']\n",
      "''\n"
     ]
    }
   ],
   "source": [
    "# 以下のようにすれば、処理時間も確認することができます。\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "def objective(x):\n",
    "    return {\n",
    "        'loss' : x ** 2,\n",
    "        'status' : STATUS_OK, # STATUS_OK の時だけlossを考慮する？\n",
    "        # -- store other results like this --\n",
    "        'eval_time' : time.time(),\n",
    "        'other_stuff' : {'type' : None, 'value' : [0,1,2]},\n",
    "        # -- attachments are handled differently\n",
    "        'attachments' : {'time_module' : pickle.dumps(time.time)}        \n",
    "    }\n",
    "\n",
    "\n",
    "trials = Trials() #計算中の戻り値を追跡可能にする。\n",
    "from hyperopt import fmin, tpe, hp\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=hp.uniform('x', -10, 10),\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=3,\n",
    "    trials=trials\n",
    "    )\n",
    "\n",
    "from pprint import pprint as print\n",
    "print('=== best ===')\n",
    "print(best)\n",
    "print('')\n",
    "print('=== trials.trials ===')\n",
    "print(trials.trials)\n",
    "print('')\n",
    "print('=== trials.losses() ===')\n",
    "print(trials.losses())\n",
    "print('')\n",
    "print('=== trials.statuses() ===')\n",
    "print(trials.statuses())\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# より複雑な探索空間を扱う場合\n",
    "\n",
    "## define an objective function\n",
    "def objective(args):\n",
    "    case, val = args\n",
    "    if case == 'case 1':\n",
    "        return val\n",
    "    else:\n",
    "        return val ** 2\n",
    "    \n",
    "print( objective(['case 2', 3]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'もし上記で定義した変数空間を確認したいなら、以下でサンプリング結果を確認することができます'\n",
      "('case 1', 5.2531236943393465)\n",
      "('case 2', -1.1447309545664304)\n",
      "('case 1', 1.671139091906336)\n",
      "('case 2', -8.76062390252624)\n",
      "('case 1', 1.113744767831003)\n",
      "('case 2', 3.7503944297243876)\n",
      "('case 2', 8.72919330433156)\n",
      "('case 1', 1.7095182325330534)\n",
      "('case 2', 1.3603761669055636)\n",
      "('case 2', -4.894127500903174)\n",
      "'--- 次に、探索を行います ---'\n",
      "100%|██████████| 100/100 [00:00<00:00, 246.51it/s, best loss: 0.01602711992416122]\n",
      "'=== best ==='\n",
      "{'a': 1, 'c2': -0.12659826193183388}\n",
      "'=== space_eval ==='\n",
      "('case 2', -0.12659826193183388)\n",
      "''\n"
     ]
    }
   ],
   "source": [
    "## 複雑な変数空間を定義する\n",
    "## define a search space of hyperparameter\n",
    "from hyperopt import hp\n",
    "space = hp.choice('a',\n",
    "                 [\n",
    "                   ('case 1', 1+hp.lognormal('c1',     0,   1)),\n",
    "                   ('case 2',          hp.uniform('c2', -10, 10))\n",
    "                 ])\n",
    "\n",
    "print('もし上記で定義した変数空間を確認したいなら、以下でサンプリング結果を確認することができます')\n",
    "import hyperopt.pyll.stochastic\n",
    "for _ in range(10):\n",
    "    print(hyperopt.pyll.stochastic.sample(space))\n",
    "\n",
    "print('--- 次に、探索を行います ---')\n",
    "\n",
    "# minimize the objective over the space\n",
    "from hyperopt import fmin, tpe, space_eval\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=100)\n",
    "\n",
    "print('=== best ===')\n",
    "print( best )\n",
    "\n",
    "print('=== space_eval ===')\n",
    "print( space_eval(space, best) )\n",
    "\n",
    "'''注釈：\n",
    "'a', 'c1', 'c2' は任意に指定できるラベルで、fminが内部的な命名に利用しています。\n",
    "上記の場合、'a'はケース名として機能し、以下のような意味合いを持ちます。\n",
    "if a == 0:\n",
    "    args = ('case 1', 1+hp.lognormal('c1',     0,   1))\n",
    "elif a == 1:\n",
    "    args = ('case 2',          hp.uniform('c2', -10, 10))\n",
    "'''\n",
    "print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1パラメータ式\n",
    "\n",
    "hyperoptの最適化アルゴリズムに用いられる変数スペースの定義は、以下のようなものがあります。\n",
    "\n",
    "## hp.choice(label, options)\n",
    "\n",
    "オプションの1つを返します。リストまたはタプルでなければなりません。 optionsの要素は、それ自体が確率的な式で [入れ子] にすることができます。この場合、いくつかのオプションにしか現れない確率的選択が 条件 パラメータになる。\n",
    "\n",
    "\n",
    "## hp.randint(label, upper)\n",
    "\n",
    "[0, upper] の範囲の乱数を返します。この分布のセマンティクスは、より遠い整数値と比較して、近くの整数値間の損失関数に相関がないことです。これは、例えばランダムシードを記述するための適切な分布です。損失関数がおそらく近くの整数値に相関する場合は、quniform 、 qloguniform 、 qnormal または qlognormal のいずれかのような「量子化」連続分布の1つを使用するべきです。\n",
    "\n",
    "\n",
    "## hp.uniform(label, low, high)\n",
    "\n",
    "low と high の間で均等に値を返します。\n",
    "最適化するとき、この変数は両側の間隔に制約されます。\n",
    "\n",
    "\n",
    "## hp.quniform(label, low, high, q)\n",
    "\n",
    "round(uniform(low, high) / q) * q のような値を返します。\n",
    "目的が幾分「滑らか」であるが、上と下の両方に拘束されるべき離散値に適している。\n",
    "\n",
    "\n",
    "## hp.loguniform(label, low, high)\n",
    "\n",
    "exp(uniform(low, high)) のような対数に一様に分布するように返します。\n",
    "最適化すると、この変数は区間 [exp(low), exp(high)] に制約されます。\n",
    "\n",
    "\n",
    "## hp.qloguniform(label, low, high, q)\n",
    "\n",
    "round(exp(uniform(low, high)) / q) * q のような値を返します。\n",
    "目的が「滑らか」で、値の大きさがより滑らかになるが、上と下の両方に限定される離散変数に適しています。\n",
    "\n",
    "\n",
    "## hp.normal(label, mu, sigma)\n",
    "\n",
    "平均ミューと標準偏差 sigma で正規分布している実際の値を返します。最適化する場合、これは制約のない変数です。\n",
    "\n",
    "\n",
    "## hp.qnormal(label, mu, sigma, q)\n",
    "\n",
    "round(normal(mu, sigma) / q) * q のような値を返します。\n",
    "おそらく mu 付近の値をとるが、根本的に無制限の離散変数に適しています。\n",
    "\n",
    "\n",
    "## hp.lognormal(label, mu, sigma)\n",
    "\n",
    "戻り値の対数が正規分布するように exp(normal(mu, sigma)) に従って描画された値を返します。最適化すると、この変数は正の値に制限されます。\n",
    "\n",
    "\n",
    "## hp.qlognormal(label, mu, sigma, q)\n",
    "\n",
    "round(exp(normal(mu, sigma)) / q) * q のような値を返します。\n",
    "目的が滑らかで、一方の側から境界を定められている変数のサイズによってスムーズになる離散変数に適しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 次に sklearn と組み合わせて hyper parameter の調整を行う。\n",
    "\n",
    "参考にしたURLは、　https://blog.amedama.jp/entry/hyperopt　です。\n",
    "\n",
    "余談ですが、英語で検索するよりも日本語で検索したほうが、良い記事が多いのはなぜだろうか。\n",
    "\n",
    "母国語である日本語での検索能力が高いからなのか、日本語の記事が優秀だからだろうか。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 25.86it/s, best loss: -0.9933333333333334]\n",
      "'--- 結果を出力する ---'\n",
      "{'C': 3.7854270724091728, 'gamma': 0.061837754861085684, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin\n",
    "from hyperopt import tpe\n",
    "from hyperopt import Trials\n",
    "from hyperopt import space_eval\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def objective(X, y, args):\n",
    "    \"\"\"最小化したい目的関数\"\"\"\n",
    "    # モデルのアルゴリズムに SVM を使う\n",
    "    model = SVC(**args)\n",
    "    # Stratified 5 Fold Cross Validation\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_validate(model, X=X, y=y, cv=kf)\n",
    "    # 最小化なので符号を反転する\n",
    "    return -1 * scores['test_score'].mean()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # データセットを読み込む\n",
    "    dataset = datasets.load_iris()\n",
    "    X, y = dataset.data, dataset.target\n",
    "    # 目的関数にデータセットを部分適用する\n",
    "    f = partial(objective, X, y)\n",
    "    # 変数の値域を定義する\n",
    "    space = {\n",
    "        # 底が自然対数なので 2.303 をかけて常用対数に変換する\n",
    "        'C': hp.loguniform('C', 2.303 * 0, 2.303 * +2),\n",
    "        'gamma': hp.loguniform('gamma', 2.303 * -2, 2.303 * +1),\n",
    "        'kernel': hp.choice('kernel', ['linear', 'rbf', 'poly']),\n",
    "    }\n",
    "    # 探索過程を記録するオブジェクト\n",
    "    trials = Trials()\n",
    "    # 目的関数を最小化するパラメータを探索する\n",
    "    best = fmin(fn=f, space=space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "    \n",
    "    print('--- 結果を出力する ---')\n",
    "    print(space_eval(space, best))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 次に最近、論文でも見かけるようになった、Train, Validation, Test の3分割データセットをつかったCrossValidationを実装してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin\n",
    "from hyperopt import tpe\n",
    "from hyperopt import Trials\n",
    "from hyperopt import space_eval\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_train_validation_test(X, y, tra_val_tes_rate=[0.6, 0.2, 0.2]):\n",
    "    # train, validation, test に三分割する\n",
    "    random_index = list(range(len(y)))\n",
    "    random.shuffle(random_index)\n",
    "    tvt_sep = [int(rate*len(y)) for rate in tra_val_tes_rate]\n",
    "    tvt_sep = [sum(tvt_sep[:i+1]) for i in range(len(tvt_sep))]\n",
    "    tvt_index = random_index[0:tvt_sep[0]], random_index[tvt_sep[0]:tvt_sep[1]], random_index[tvt_sep[1]:tvt_sep[2]]\n",
    "    train_X, train_y = X[tvt_index[0]], y[tvt_index[0]]\n",
    "    validation_X, validation_y = X[tvt_index[1]], y[tvt_index[1]]\n",
    "    test_X, test_y  = X[tvt_index[2]], y[tvt_index[2]]\n",
    "    return train_X, train_y, validation_X, validation_y, test_X, test_y\n",
    "\n",
    "def objective(train_X, train_y, val_X, val_y, args):\n",
    "    \"\"\"最小化したい目的関数\"\"\"\n",
    "    # モデルのアルゴリズムに SVM を使う\n",
    "    model = SVC(**args)\n",
    "    model.fit(train_X, train_y)\n",
    "    predict = model.predict(val_X)\n",
    "    errors = predict - val_y    \n",
    "    # 最小化なので符号を反転する\n",
    "    return np.abs(errors).mean()\n",
    "\n",
    "\n",
    "# データセットを読み込む\n",
    "dataset = datasets.load_iris()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "train_X, train_y, validation_X, validation_y, test_X, test_y = split_train_validation_test(X, y)\n",
    "\n",
    "# 目的関数にデータセットを適応する\n",
    "f = partial(objective, train_X, train_y, validation_X, validation_y)\n",
    "# 変数の値域を定義する\n",
    "space = {\n",
    "    # 底が自然対数なので 2.303 をかけて常用対数に変換する\n",
    "    'C': hp.loguniform('C', 2.303 * 0, 2.303 * +2),\n",
    "    'gamma': hp.loguniform('gamma', 2.303 * -2, 2.303 * +1),\n",
    "    'kernel': hp.choice('kernel', ['linear', 'rbf', 'poly']),\n",
    "}\n",
    "# 探索過程を記録するオブジェクト\n",
    "trials = Trials()\n",
    "# 目的関数を最小化するパラメータを探索する\n",
    "best = fmin(fn=f, space=space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "\n",
    "print('--- 最適なハイパーパラメータを確認する ---')\n",
    "print(space_eval(space, best))\n",
    "print('--- テストスコアを計算する ---')\n",
    "model = SVC(**space_eval(space, best))\n",
    "model.fit(train_X, train_y)\n",
    "predict = model.predict(test_X)\n",
    "errors = predict - test_y    \n",
    "print(np.abs(errors).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 変数探索に使われるalgoの種類について\n",
    "\n",
    "\n",
    "公式ドキュメント （https://hyperopt.github.io/hyperopt/）　によれば、現在(2019/3/30)実装されている探索アルゴリズムは下記の２つのみ。\n",
    "\n",
    "1. Random Search\n",
    "2. Tree of Parzen Estimators (TPE)\n",
    "\n",
    "Gaussian processesやregression treeをに基づいた混合ベイズ最適化モデル？（accommodate Bayesian optimization algorithms）というものがある。これに基づいた実装としてhyperopt は開発されたが、まだ実装されてはいない。\n",
    "\n",
    "Random Search についてはここでは言及しない。なぜなら、簡単だと思うから。\n",
    "\n",
    "TPE　について言及します。\n",
    "\n",
    "\n",
    "## パラメータの自動調整技術の変遷\n",
    "\n",
    "[NeuPy] によると、パラメータの自動調整の技術の変遷は、SMBO　-> gaussian process -> TPE となっているそうです。\n",
    "\n",
    "TPEは2011年にNIPSで発表された論文(https://www.lri.fr/~kegl/research/PDFs/BeBaBeKe11.pdf)です。\n",
    "\n",
    "[NeuPy] http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html\n",
    "\n",
    "\n",
    "## NeuPy の記事を読んでみよう。\n",
    "\n",
    "NeuPy は Neural Networks のパラメータ最適化モジュールです。自動調整の技術の変遷と概要をまとめた記事があったので、簡単に和訳していきます。\n",
    "\n",
    "詳細については、上記の[NeuPy]のURLを参照してください。\n",
    "\n",
    "\n",
    "### 以下、和訳で要約\n",
    "\n",
    "パラメータの自動調整の種類を、以下に列挙します。\n",
    "\n",
    "1. Grid Search\n",
    "2. Random Search\n",
    "3. Hund-tuning\n",
    "4. Gaussian Process with Expected Improvement\n",
    "5. Tree-Structured Parzen Estimators (TPE)\n",
    "\n",
    "ここでは、4.　と 5. について該当する記事を和訳しておきます。\n",
    "\n",
    "\n",
    "#### Gaussian Process with Expected Improvement\n",
    "\n",
    "(記事の場所：http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#bayesian-optimization)\n",
    "\n",
    "\n",
    "* Bayesian Optimization\n",
    "\n",
    "Bayesian Optimization　は　派生のある？（derivative-free） の最適化法です。いくつかの最適化タイプがありますが、ここでは　Acquisition Function を使った Gaussian Process に注目します。前述で紹介した　Hand-tuning での説明と似ていると思われるかもしれません。　Gaussian Process は前回のパラメータと精度の結果を使い、次の未観測パラメータを仮説します。Axquisition Function はこれらの情報を使い、次のパラメータセットを提案します。\n",
    "\n",
    "* Gaussian Process\n",
    "\n",
    "(内田コメント)　Gaussian Process の一般的理解がないと、以下を読み解くのは難しかった。これ以降でもsklearnのGaussian Processモジュールを応用した実装を行うので、下記をあらかじめ読んでおくべき。\n",
    "\n",
    "[sklearnの解説記事]　\n",
    "\n",
    "https://scikit-learn.org/stable/modules/gaussian_process.html\n",
    "\n",
    "[sklearnの実装レファレンス] \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor\n",
    "\n",
    "[内田ではよく分からなかったけど、数理的に説明している日本語記事] \n",
    "\n",
    "https://qiita.com/kilometer/items/6ab1d7f2cffc0c6e4a95\n",
    "\n",
    "[頭のいい人が「分かりやすい」と言っていたスライド資料。その１]　\n",
    "\n",
    "https://www.ism.ac.jp/~daichi/lectures/H26-GaussianProcess/gp-lecture1-matsui.pdf\n",
    "\n",
    "[頭のいい人が「分かりやすい」と言っていたスライド資料。その２]　　←　オススメ！\n",
    "\n",
    "https://www.ism.ac.jp/~daichi/lectures/H26-GaussianProcess/gp-lecture2-daichi.pdf\n",
    "\n",
    "\n",
    "\n",
    "Gaussian Process の背景には、インプット x は　アウトプット　y　= f(x) を持ち、f は確率関数である、という考えがあります。この確率関数はアウトプットをgaussian分布からサンプリングします。また、インプット　x はgaussian分布に帰属しているといえます。つまり、インプットxを生成されるgaussian　proccessは、いくつかのgaussian分布の平均μと標準偏差σを定義します。\n",
    "\n",
    "Gaussian Process は　Multivariate Gaussian Distribution(MGD) に基づいた生成です。MGDは平均ベクトルと共分散ベクトルで定義されます。一方で、Gaussian Process は平均関数(mean function)と共分散関数（covariance function）で定義されます。基本的には、関数は無限ベクトルです（取り得る値が無限で、ベクトルで返却する関数ということ？）。また、多変量ガウス分布は離散的な数の可能な入力を持つ関数に対するガウス過程であると言えます。\n",
    "\n",
    "例えば、二次元のパラメータベクトル　x　を多変量ガウス分布で生成する場合、以下のようなサンプリングとなります。\n",
    "\n",
    "![MGDE01](img/MGDE01.png)\n",
    "\n",
    "これを違う可視化にすると、下記のようになります。結ばれた線は同時にサンプリングされたことを示しています。このグラフであれば、パラメータベクトルの次元を増やしても表記することができます。\n",
    "\n",
    "![MGDE02](img/MGDE02.png)\n",
    "\n",
    "パラメータベクトルが5次元で、十分な回数サンプリングすれば下記のようになります。\n",
    "\n",
    "![MGDE03](img/MGDE03.png)\n",
    "\n",
    "つまり、最適なパラメータベクトルをサンプリングするためには、この多変量ガウス分布の平均ベクトルと共分散行列をなんらかの学習過程で推定し、収束させれば良いのです。\n",
    "\n",
    "例えば、３層のニューラルネットワークの中間hidden層のニューロン数をパラメータであるとしましょう。\n",
    "ニューロン数を2回サンプリングして、それぞれの精度を観測します。\n",
    "すると下図のような、パラメータ（X軸）と精度（Y軸）の予測が更新されます。\n",
    "後は、iterationを重ねて、サンプリングすべき範囲を狭めていきます。\n",
    "つまり、下記のようになります。\n",
    "\n",
    "![MGDE04](img/MGDE04.png)\n",
    "\n",
    "しかし、前述のGausian Processでは分散を設定しているので、上図のように観測点が一点にはなりません。その精度を決める予測値は決定関数から得られたものであり、その決定関数はほとんどのneural networkにとって正しくないのです(注釈1)。観測されるノイズを決定しているGausian Processの分散パラメータを変更することで、これを修正する。この工夫はそれほど確かではない予測だけでなく、観測データ点を通過しない隠れユニット数の平均を与えます。\n",
    "\n",
    "\n",
    "[注釈1] この意味が分かる人がいればフォローをお願いします。内田の解釈では、パラメータが決まった状態で学習した結果を、決定関数（a deterministic function）と表現しているのだと思います。決定関数がテストセットの変数から予測値を出力します。そして、予測値とテストセットの正解値から誤差を計算して精度指標（Accuracy）が計算されます。よって、精度指標はパラメータが固定されていても、テストセットと学習結果の重みが確率的に変動すれば分散します。テストセットは一般的に乱数抽出されますし、重みも学習過程に乱数要素を含みます。よって、観測点においても精度は一意に決まらず分散します。\n",
    "\n",
    "![MGDE05](img/MGDE05.png)\n",
    "\n",
    "\n",
    "* Acquisition Function\n",
    "\n",
    "Acquisition Function は次のステップのパラメータセットを決定します。その種類は複数あります。その内の代表的なものが、Expected Improvement です。もし、最小化を目的として次のパラメータセットを探索する場合は、次の数式を用います。\n",
    "\n",
    "\\begin{align}\n",
    "g_{min}(x)  = max(0, y_{min} - y_{lowest\\ expected}) \\\\ \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "y_{min} : 観測されたyの最小値\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "y_{lowest\\ expected} : 各xに関連づけられた信頼区間内の最小値。\n",
    "\\end{align}\n",
    "\n",
    "今回の隠れ層のユニット数を探索する場合は、Accuracyの最大化問題なので、以下を用います。\n",
    "\n",
    "\\begin{align}\n",
    "g_{max}(x)  = max(0, y_{highest\\ expected} - y_{max}) \\\\ \n",
    "\\end{align}\n",
    "\n",
    "このExpected Improvement function に従えば、各ステップの次のxは下記のように選出されます。\n",
    "\n",
    "![MGDE06](img/MGDE06.png)\n",
    "\n",
    "\n",
    "* Find number of hidden units\n",
    "\n",
    "それでは、Gaussian Process regression　と　Expected Improvement　function に基づいたハイパーパラメータの探索を実装してみます。ひきつづき、最適な隠れユニット数を探索する問題にとりくみます。おなじみの手書き数値の判別問題です。\n",
    "\n",
    "まずは、学習、ニューラルネットワーク、予測誤差を関数で定義しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy への依存部分にエラーがあり、importできなかった。\n",
    "# とはいえ、コードに目を通すだけでプロセスを理解することはできるので問題ないはず。\n",
    "from neupy import algorithms, layers \n",
    "\n",
    "################################\n",
    "# ニューラルネットワークの定義とその学習。戻り値はテストセットの精度スコアになっている。\n",
    "def train_network(n_hidden, x_train, x_test, y_train, y_test):\n",
    "    network = algorithms.Momentum(\n",
    "        [\n",
    "            layers.Input(64),\n",
    "            layers.Relu(n_hidden),\n",
    "            layers.Softmax(10),\n",
    "        ],\n",
    "\n",
    "        # Randomly shuffle dataset before each\n",
    "        # training epoch.\n",
    "        shuffle_data=True,\n",
    "\n",
    "        # Do not show training progress in output\n",
    "        verbose=False,\n",
    "\n",
    "        step=0.001,\n",
    "        batch_size=128,\n",
    "        loss='categorical_crossentropy',\n",
    "    )\n",
    "    network.train(x_train, y_train, epochs=100)\n",
    "\n",
    "    # Calculates categorical cross-entropy error between\n",
    "    # predicted value for x_test and y_test value\n",
    "    return network.score(x_test, y_test)\n",
    "\n",
    "\n",
    "################################\n",
    "# 次に手書きデータセットをscikit-learnで読み込む。\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from neupy import utils\n",
    "\n",
    "utils.reproducible()\n",
    "\n",
    "dataset = datasets.load_digits()\n",
    "n_samples = dataset.target.size\n",
    "n_classes = 10\n",
    "\n",
    "# One-hot encoder\n",
    "target = np.zeros((n_samples, n_classes))\n",
    "target[np.arange(n_samples), dataset.target] = 1\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, target, test_size=0.3\n",
    ")\n",
    "\n",
    "\n",
    "################################\n",
    "# パラメータの選択プロセスを定義する。\n",
    "# はじめに、Gaussian Process regression を実行し特定のベクトル入力に対して予測の平均と標準偏差を返却する関数を定義します。\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcess\n",
    "\n",
    "def vector_2d(array):\n",
    "    return np.array(array).reshape((-1, 1))\n",
    "\n",
    "def gaussian_process(x_train, y_train, x_test):\n",
    "    '''\n",
    "    上記の引数名だと、まるで手書きデータセットを受け取ると誤解してしまう。\n",
    "    実際に引数にとるのは、下記のデータである。\n",
    "    \n",
    "    x_train = 検証済みのパラメータが各ステップごとに格納されている \n",
    "    y_train = 検証結果の精度スコアが各ステップごとに格納されている\n",
    "    x_test　= 次のステップで選択されるパラメータの候補が格納されている\n",
    "    '''\n",
    "    x_train = vector_2d(x_train)\n",
    "    y_train = vector_2d(y_train)\n",
    "    x_test = vector_2d(x_test)\n",
    "\n",
    "    # Train gaussian process\n",
    "    gp = GaussianProcess(corr='squared_exponential',\n",
    "                         theta0=1e-1, thetaL=1e-3, thetaU=1)\n",
    "    gp.fit(x_train, y_train)\n",
    "\n",
    "    # Get mean and standard deviation for each possible\n",
    "    # number of hidden units\n",
    "    # 取り得る隠れユニット数ごとの平均と標準偏差を取得します。\n",
    "    y_mean, y_var = gp.predict(x_test, eval_MSE=True)\n",
    "    y_std = np.sqrt(vector_2d(y_var))\n",
    "\n",
    "    return y_mean, y_std\n",
    "\n",
    "\n",
    "####################################\n",
    "#  予測結果を　Expected Improvement(EI) に適応し、次の最適ステップを探索します。\n",
    "\n",
    "def next_parameter_by_ei(y_min, y_mean, y_std, x_choices):\n",
    "    # Calculate expecte improvement from 95% confidence interval\n",
    "    expected_improvement = y_min - (y_mean - 1.96 * y_std)\n",
    "    expected_improvement[expected_improvement < 0] = 0\n",
    "\n",
    "    max_index = expected_improvement.argmax()\n",
    "    # Select next choice\n",
    "    next_parameter = x_choices[max_index]\n",
    "\n",
    "    return next_parameter\n",
    "\n",
    "\n",
    "####################################\n",
    "# 最後に、上記で定義したプロシージャを一つの関数にまとめます。\n",
    "\n",
    "import random\n",
    "\n",
    "def hyperparam_selection(func, n_hidden_range, func_args=None, n_iter=20):\n",
    "    ''' 使用例\n",
    "    best_n_hidden = hyperparam_selection(\n",
    "        train_network,　# 前述で定義した関数\n",
    "        n_hidden_range=[50, 1000],\n",
    "        func_args=[x_train, x_test, y_train, y_test],\n",
    "        n_iter=6,)\n",
    "    '''\n",
    "    if func_args is None:\n",
    "        func_args = []\n",
    "\n",
    "    scores = []\n",
    "    parameters = []\n",
    "\n",
    "    min_n_hidden, max_n_hidden = n_hidden_range\n",
    "    n_hidden_choices = np.arange(min_n_hidden, max_n_hidden + 1)\n",
    "\n",
    "    # To be able to perform gaussian process we need to\n",
    "    # have at least 2 samples.\n",
    "    # gaussian processをとりあえず稼働するためのn_hiddenを乱数抽出します。\n",
    "    n_hidden = random.randint(min_n_hidden, max_n_hidden)\n",
    "    score = func(n_hidden, *func_args)\n",
    "    \n",
    "    # parameters に検証したn_hiddenを追加。そして、その精度スコアをscoresに追加。\n",
    "    parameters.append(n_hidden)\n",
    "    scores.append(score)\n",
    "\n",
    "    n_hidden = random.randint(min_n_hidden, max_n_hidden)\n",
    "\n",
    "    for iteration in range(2, n_iter + 1):\n",
    "        score = func(n_hidden, *func_args)\n",
    "\n",
    "        parameters.append(n_hidden)\n",
    "        scores.append(score)\n",
    "        \n",
    "        # 検証済みのparameters と score に、gaussian_processを実行して次のステップで検証すべきn_hiddenを選ぶ。\n",
    "        y_min = min(scores)\n",
    "        y_mean, y_std = gaussian_process(parameters, scores,\n",
    "                                         n_hidden_choices)\n",
    "\n",
    "        n_hidden = next_parameter_by_ei(y_min, y_mean, y_std,\n",
    "                                        n_hidden_choices)\n",
    "\n",
    "        if y_min == 0 or n_hidden in parameters:\n",
    "            # Lowest expected improvement value have been achieved\n",
    "            break\n",
    "\n",
    "    # 結果、精度スコアが最小値となったパラメータを返却する\n",
    "    min_score_index = np.argmin(scores)\n",
    "    return parameters[min_score_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のプログラムで定義された手続きのイメージは、以下のようなものである。\n",
    "\n",
    "![MGDE07](img/MGDE07.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
